# Phase 8 å®Ÿè£…ä»•æ§˜æ›¸

**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå**: LlmMultiChat3  
**ãƒ•ã‚§ãƒ¼ã‚º**: Phase 8 - LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æœ€çµ‚çµ±åˆ  
**æœŸé–“**: 3é€±é–“  
**ä½œæˆæ—¥**: 2025-11-20  
**Phase 7å®Œäº†å‰æ**: 3Då¯è¦–åŒ–ãƒ»è‡ªå¾‹ã‚µãƒ¼ãƒå®Ÿè£…æ¸ˆã¿

---

## ç›®æ¬¡

1. [Phase 8æ¦‚è¦](#1-phase-8æ¦‚è¦)
2. [å‰ææ¡ä»¶](#2-å‰ææ¡ä»¶)
3. [Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#3-week-1-2-loraãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
4. [Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼](#4-week-3-æœ€çµ‚çµ±åˆå“è³ªä¿è¨¼)
5. [æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯](#5-æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯)
6. [ãƒ†ã‚¹ãƒˆè¨ˆç”»](#6-ãƒ†ã‚¹ãƒˆè¨ˆç”»)
7. [æˆæœç‰©](#7-æˆæœç‰©)

---

## 1. Phase 8æ¦‚è¦

### 1.1 ç›®çš„

LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é€²åŒ–ã¨å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã«ã‚ˆã‚Šã€**å®Œå…¨ãªä¼šè©±AIã‚·ã‚¹ãƒ†ãƒ **ã‚’å®Œæˆã•ã›ã¾ã™ã€‚

### 1.2 ä¸»è¦æ©Ÿèƒ½

| æ©Ÿèƒ½ã‚«ãƒ†ã‚´ãƒª | èª¬æ˜ | Priority |
|-------------|------|----------|
| **LoRAé©ç”¨** | æœˆæ¬¡ãƒãƒƒãƒå‡¦ç† | ğŸŸ¡ Medium |
| **çµ±åˆãƒ†ã‚¹ãƒˆ** | Phase 1-7æ¨ªæ–­ãƒ†ã‚¹ãƒˆ | ğŸ”´ High |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–** | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ”¹å–„ | ğŸ”´ High |
| **ãƒªãƒªãƒ¼ã‚¹æº–å‚™** | ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† | ğŸ”´ High |

### 1.3 é”æˆç›®æ¨™

âœ… LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ä½œç¢ºèª  
âœ… å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ  
âœ… ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†  
âœ… v4.0.0ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆä½œæˆ

---

## 2. å‰ææ¡ä»¶

### 2.1 Phase 1-7å®Œäº†äº‹é …

âœ… **Phase 1**: LangGraphã‚³ã‚¢ãƒ»5éšå±¤è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ   
âœ… **Phase 2**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£  
âœ… **Phase 3**: REST/WebSocket APIï¼ˆ23ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰  
âœ… **Phase 4**: é€£æƒ³è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãƒ»æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«åŸºç›¤  
âœ… **Phase 5**: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œãƒ»è‡ªå·±çœå¯Ÿ  
âœ… **Phase 6**: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ãƒ»MCP Server  
âœ… **Phase 7**: 3Då¯è¦–åŒ–ãƒ»è‡ªå¾‹ã‚µãƒ¼ãƒ

**å‚ç…§**: [`docks/å®Ÿè£…ä»•æ§˜/Phase7_å®Ÿè£…ä»•æ§˜.md`](Phase7_å®Ÿè£…ä»•æ§˜.md:1)

### 2.2 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

**LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨**:
- **GPU**: VRAM 8GBä»¥ä¸Šï¼ˆNVIDIAæ¨å¥¨ï¼‰
- **CPUä»£æ›¿**: å¯èƒ½ã ãŒ10å€ä»¥ä¸Šã®æ™‚é–“
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Š

---

## 3. Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 3.1 å®Ÿè£…å†…å®¹

**å‚ç…§**: [`docks/ä»•æ§˜æ›¸/03_ä¼šè©±LLM_ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»•æ§˜.md:335-379`](../ä»•æ§˜æ›¸/03_ä¼šè©±LLM_ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»•æ§˜.md:335)

#### 3.1.1 æœˆæ¬¡ãƒãƒƒãƒå‡¦ç†

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
1. **ä¼šè©±å±¥æ­´åé›†**: éå»1ãƒ¶æœˆåˆ†ã®å…¨ä¼šè©±
2. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ**: Alpacaå½¢å¼å¤‰æ›
3. **LoRAé©ç”¨**: PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§å­¦ç¿’
4. **ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: `models/lora_{character_name}/`

#### 3.1.2 è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢å¼

```json
{
  "instruction": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚„æŒ‡ç¤º",
  "input": "è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
  "output": "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å¿œç­”"
}
```

**ä¾‹**:
```json
{
  "instruction": "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
  "input": "",
  "output": "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä¸»ã«æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚"
}
```

### 3.2 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

#### training/lora_tuning.py (400è¡Œ)

```python
"""LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«."""

from typing import List, Dict, Any
import os
import json
from datetime import datetime, timedelta
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

class CharacterFineTuning:
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹."""
    
    def __init__(
        self,
        base_model: str = "gpt2",
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.1
    ):
        """
        åˆæœŸåŒ–.
        
        Args:
            base_model: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å
            lora_r: LoRAãƒ©ãƒ³ã‚¯
            lora_alpha: LoRAã‚¢ãƒ«ãƒ•ã‚¡
            lora_dropout: LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        """
        self.base_model_name = base_model
        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["c_attn"],
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
    
    def fine_tune_character(
        self,
        character_name: str,
        num_epochs: int = 3,
        batch_size: int = 4
    ) -> str:
        """
        ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        Returns:
            str: ä¿å­˜ãƒ‘ã‚¹
        """
        # 1. ä¼šè©±å±¥æ­´åé›†
        conversations = self._collect_conversations(character_name, days=30)
        
        if len(conversations) < 100:
            raise ValueError(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(conversations)}ä»¶ï¼ˆæœ€ä½100ä»¶å¿…è¦ï¼‰")
        
        # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        training_data = self._create_training_data(conversations)
        
        # 3. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
        model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4. LoRAé©ç”¨
        model = get_peft_model(model, self.lora_config)
        
        # 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = Dataset.from_list(training_data)
        
        def tokenize_function(examples):
            prompts = [
                f"Instruction: {inst}\nInput: {inp}\nOutput: {out}"
                for inst, inp, out in zip(
                    examples["instruction"],
                    examples["input"],
                    examples["output"]
                )
            ]
            return tokenizer(
                prompts,
                truncation=True,
                padding="max_length",
                max_length=512
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # 6. è¨“ç·´è¨­å®š
        output_dir = f"models/lora_{character_name}"
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            save_steps=100,
            logging_steps=10,
            learning_rate=3e-4,
            fp16=torch.cuda.is_available(),
        )
        
        # 7. è¨“ç·´å®Ÿè¡Œï¼ˆå®Ÿéš›ã¯Trainerã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰
        # trainer = Trainer(...)
        # trainer.train()
        
        # 8. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        return output_dir
    
    def _collect_conversations(
        self,
        character_name: str,
        days: int
    ) -> List[Dict[str, Any]]:
        """
        ä¼šè©±å±¥æ­´åé›†.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            days: åé›†æ—¥æ•°
        
        Returns:
            List[Dict]: ä¼šè©±å±¥æ­´
        """
        from services.chat_service import ChatService
        
        chat_service = ChatService()
        
        # éå»Næ—¥åˆ†ã®ä¼šè©±å–å¾—
        start_date = datetime.utcnow() - timedelta(days=days)
        
        conversations = chat_service.get_history(
            character_name=character_name,
            start_date=start_date,
            limit=10000
        )
        
        return conversations
    
    def _create_training_data(
        self,
        conversations: List[Dict[str, Any]]
    ) -> List[Dict[str, str]]:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆAlpacaå½¢å¼ï¼‰.
        
        Args:
            conversations: ä¼šè©±å±¥æ­´
        
        Returns:
            List[Dict]: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        """
        training_data = []
        
        for conv in conversations:
            # user â†’ assistant ã®ãƒšã‚¢æŠ½å‡º
            if conv.get("role") == "user":
                user_input = conv.get("content", "")
                
                # æ¬¡ã®assistantå¿œç­”ã‚’æ¢ã™
                assistant_output = None
                for i, c in enumerate(conversations):
                    if c.get("timestamp") > conv.get("timestamp") and c.get("role") == "assistant":
                        assistant_output = c.get("content")
                        break
                
                if assistant_output:
                    training_data.append({
                        "instruction": user_input,
                        "input": "",
                        "output": assistant_output
                    })
        
        return training_data
    
    def load_lora_model(self, character_name: str):
        """
        LoRAãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
        
        Returns:
            Model: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        """
        from peft import PeftModel
        
        model_path = f"models/lora_{character_name}"
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"LoRAãƒ¢ãƒ‡ãƒ«ãªã—: {model_path}")
        
        base_model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
        model = PeftModel.from_pretrained(base_model, model_path)
        
        return model
```

### 3.3 ãƒ†ã‚¹ãƒˆï¼ˆ8ä»¶ - GPUä¸è¦ãªè»½é‡ãƒ†ã‚¹ãƒˆï¼‰

#### tests/test_lora_tuning.py

```python
"""LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ç‰ˆï¼‰."""

import pytest
from training.lora_tuning import CharacterFineTuning


def test_training_data_creation():
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆ."""
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯", "timestamp": "2025-11-20T10:00:00Z"},
        {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼", "timestamp": "2025-11-20T10:00:01Z"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    assert len(data) == 1
    assert data[0]["instruction"] == "ã“ã‚“ã«ã¡ã¯"
    assert data[0]["output"] == "ã“ã‚“ã«ã¡ã¯ï¼"


def test_lora_config():
    """LoRAè¨­å®šãƒ†ã‚¹ãƒˆ."""
    tuning = CharacterFineTuning(lora_r=8, lora_alpha=16)
    
    assert tuning.lora_config.r == 8
    assert tuning.lora_config.lora_alpha == 16


@pytest.mark.skipif(not pytest.config.getoption("--run-gpu"), reason="GPU test")
def test_model_save_load():
    """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆGPUå¿…è¦ï¼‰."""
    # GPUãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã®ã¿å®Ÿè¡Œ
    pass


# ... ä»–5ä»¶ï¼ˆè»½é‡ãƒ†ã‚¹ãƒˆã®ã¿ï¼‰
```

---

## 4. Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼

### 4.1 å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆ

#### tests/test_integration_phase4_8.py

```python
"""Phase 4-8çµ±åˆãƒ†ã‚¹ãƒˆ."""

import pytest
from services.chat_service import ChatService
from memory.associative import AssociativeMemory
from core.emotion import EmotionalState
from core.dialogue_style import AdaptiveDialogueStyle
from core.character_growth import CharacterGrowth
from api.mcp_server import LlmMultiChatMCPServer


@pytest.mark.asyncio
async def test_end_to_end_conversation():
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ä¼šè©±ãƒ†ã‚¹ãƒˆ."""
    chat_service = ChatService()
    
    # ä¼šè©±å®Ÿè¡Œ
    response = await chat_service.chat(
        user_input="æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        character_name="lumina",
        session_id="integration_test_001"
    )
    
    assert response["status"] == "success"
    assert len(response["response"]) > 0


def test_associative_memory_integration():
    """é€£æƒ³è¨˜æ†¶çµ±åˆãƒ†ã‚¹ãƒˆ."""
    memory = AssociativeMemory(db_path=":memory:")
    
    # æ¦‚å¿µè¿½åŠ 
    memory.add_concept("Python", embedding=[0.1]*128, metadata={})
    memory.add_concept("æ©Ÿæ¢°å­¦ç¿’", embedding=[0.2]*128, metadata={})
    memory.link_concepts("Python", "æ©Ÿæ¢°å­¦ç¿’", "related", strength=0.8)
    
    # é€£æƒ³æ¤œç´¢
    results = memory.retrieve_associated_concepts("Python", depth=2, threshold=0.5)
    
    assert len(results) > 0


def test_character_growth_with_kpi():
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·KPIçµ±åˆãƒ†ã‚¹ãƒˆ."""
    growth = CharacterGrowth("lumina", db_path=":memory:")
    
    # KPIæ›´æ–°
    for _ in range(10):
        growth.update_kpi("user_thumbs_up", value=1)
    
    # ãƒ¬ãƒ™ãƒ«ç¢ºèª
    assert growth.current_level == 1


@pytest.mark.asyncio
async def test_mcp_server_integration():
    """MCP Serverçµ±åˆãƒ†ã‚¹ãƒˆ."""
    server = LlmMultiChatMCPServer()
    
    result = await server._chat_with_character("lumina", "ã“ã‚“ã«ã¡ã¯")
    
    assert len(result) > 0


# ... ä»–çµ±åˆãƒ†ã‚¹ãƒˆ
```

### 4.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### profiler/performance_analysis.py

```python
"""ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ."""

import time
import cProfile
import pstats
from io import StringIO


def profile_chat_service():
    """ChatServiceãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«."""
    profiler = cProfile.Profile()
    
    profiler.enable()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    from services.chat_service import ChatService
    chat_service = ChatService()
    
    for _ in range(10):
        chat_service.chat(
            user_input="ãƒ†ã‚¹ãƒˆ",
            character_name="lumina",
            session_id="bench_001"
        )
    
    profiler.disable()
    
    # çµæœå‡ºåŠ›
    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    ps.print_stats(10)
    
    print(s.getvalue())
```

### 4.3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ€çµ‚æ›´æ–°

**ä½œæˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
1. **å®Œå…¨ä»•æ§˜æ›¸æ›´æ–°**: å…¨Phaseçµ±åˆå†…å®¹
2. **APIä»•æ§˜æ›¸æœ€çµ‚ç‰ˆ**: å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§
3. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ä½¿ç”¨æ–¹æ³•
4. **ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0**: å¤‰æ›´å±¥æ­´

### 4.4 ãƒªãƒªãƒ¼ã‚¹æº–å‚™

#### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0

```markdown
# LlmMultiChat3 v4.0.0 ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ

**ãƒªãƒªãƒ¼ã‚¹æ—¥**: 2025å¹´XXæœˆXXæ—¥

## ğŸ‰ æ–°æ©Ÿèƒ½

### Phase 4: è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ + æ„Ÿæƒ…åŸºç›¤
- é€£æƒ³è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSQLite Graphï¼‰
- 8åŸºæœ¬æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«ï¼ˆPlutchikï¼‰

### Phase 5: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œ + è‡ªå·±çœå¯Ÿ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«è‡ªå‹•èª¿æ•´
- è‡ªå·±çœå¯Ÿãƒ»çŸ›ç›¾æ¤œå‡º

### Phase 6: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•· + MCPå¯¾å¿œ
- KPIãƒ™ãƒ¼ã‚¹æˆé•·ã‚·ã‚¹ãƒ†ãƒ 
- MCP Serverå®Ÿè£…ï¼ˆClaude Desktopçµ±åˆï¼‰

### Phase 7: 3Då¯è¦–åŒ– + è‡ªå¾‹ã‚µãƒ¼ãƒ
- é€£æƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯3Då¯è¦–åŒ–ï¼ˆPlotly.jsï¼‰
- è‡ªå¾‹çš„å¤–éƒ¨ã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

### Phase 8: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æœ€çµ‚çµ±åˆ
- æœˆæ¬¡ãƒãƒƒãƒLoRAå­¦ç¿’
- å…¨Phaseçµ±åˆå®Œäº†

## ğŸ“Š æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

- **Backend**: Python 3.9+, FastAPI, LangGraph
- **Frontend**: React 18, TypeScript, Vite
- **Database**: SQLite, Redis
- **AI**: OpenAI API, Ollama
- **Visualization**: Plotly.js

## ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
git clone https://github.com/Nyukimin/LlmMultiChat3.git
cd LlmMultiChat3
pip install -r requirements.txt
uvicorn main:app --reload
```

## ğŸ“– ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [å®Œå…¨ä»•æ§˜æ›¸](docks/ä»•æ§˜æ›¸/)
- [APIä»•æ§˜æ›¸](docks/APIä»•æ§˜æ›¸.md)
- [ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰](docks/ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰.md)

## ğŸ› æ—¢çŸ¥ã®å•é¡Œ

- LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯GPUæ¨å¥¨ï¼ˆCPUç‰ˆã¯ä½é€Ÿï¼‰

## ğŸ‘ è²¢çŒ®è€…

- @Nyukimin
```

---

## 5. æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### 5.1 Pythonä¾å­˜

```txt
# requirements.txt ã«è¿½åŠ 
transformers==4.35.0    # LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
peft==0.6.0             # PEFTï¼ˆLoRAï¼‰
torch==2.1.0            # PyTorch
datasets==2.15.0        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
```

### 5.2 æ–°è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

- **training/lora_tuning.py**: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

---

## 6. ãƒ†ã‚¹ãƒˆè¨ˆç”»

### 6.1 ãƒ†ã‚¹ãƒˆæ§‹æˆ

| ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« | ãƒ†ã‚¹ãƒˆä»¶æ•° | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ |
|---------------|-----------|---------------|
| `tests/test_lora_tuning.py` | 8ä»¶ | > 70% |
| `tests/test_integration_phase4_8.py` | 10ä»¶ | > 80% |
| **åˆè¨ˆ** | **18ä»¶** | **> 75%** |

### 6.2 å®Ÿè¡Œæ–¹æ³•

```bash
# è»½é‡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/test_lora_tuning.py -v

# çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/test_integration_phase4_8.py -v

# LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œï¼ˆGPUæ¨å¥¨ï¼‰
python -m training.lora_tuning --character lumina --epochs 3
```

---

## 7. æˆæœç‰©

### 7.1 å®Ÿè£…ã‚³ãƒ¼ãƒ‰

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**:
- `training/lora_tuning.py` (400è¡Œ)
- **åˆè¨ˆ**: 400è¡Œ

### 7.2 ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**:
- `tests/test_lora_tuning.py` (8ä»¶)
- `tests/test_integration_phase4_8.py` (10ä»¶)
- **åˆè¨ˆ**: 18ä»¶

### 7.3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- `docks/å®Œäº†å ±å‘Š/Phase8_å®Œäº†ã‚µãƒãƒªãƒ¼.md`
- `docks/å®Œäº†å ±å‘Š/Phase4-8_æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ.md`
- ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0

### 7.4 ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³

- [ ] LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ä½œç¢ºèª
- [ ] å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ
- [ ] ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†
- [ ] å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼ˆ18ä»¶ï¼‰
- [ ] v4.0.0ãƒªãƒªãƒ¼ã‚¹

---

**Phase 8 å®Ÿè£…å®Œäº†**: LlmMultiChat3 v4.0.0å®Œæˆï¼