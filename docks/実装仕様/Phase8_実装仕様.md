# Phase 8 å®Ÿè£…ä»•æ§˜æ›¸

**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå**: LlmMultiChat3  
**ãƒ•ã‚§ãƒ¼ã‚º**: Phase 8 - LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æœ€çµ‚çµ±åˆ  
**æœŸé–“**: 3é€±é–“  
**ä½œæˆæ—¥**: 2025-11-20  
**Phase 7å®Œäº†å‰æ**: 3Då¯è¦–åŒ–ãƒ»è‡ªå¾‹ã‚µãƒ¼ãƒå®Ÿè£…æ¸ˆã¿

---

## ç›®æ¬¡

1. [Phase 8æ¦‚è¦](#1-phase-8æ¦‚è¦)
2. [å‰ææ¡ä»¶](#2-å‰ææ¡ä»¶)
3. [Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](#3-week-1-2-loraãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°)
4. [Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼](#4-week-3-æœ€çµ‚çµ±åˆå“è³ªä¿è¨¼)
5. [æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯](#5-æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯)
6. [ãƒ†ã‚¹ãƒˆè¨ˆç”»ï¼ˆTDDå®Ÿè£…ï¼‰](#6-ãƒ†ã‚¹ãƒˆè¨ˆç”»tddå®Ÿè£…)
   - [TDDå®Ÿè£…ä»•æ§˜ã‚µãƒãƒªãƒ¼](#60-tddå®Ÿè£…ä»•æ§˜ã‚µãƒãƒªãƒ¼)
   - [ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™](#61-ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™)
   - [ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•](#62-ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•)
   - [Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - ãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰](#week-1-2-loraãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°---ãƒ†ã‚¹ãƒˆä»•æ§˜tdd)
   - [Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼ - ãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰](#week-3-æœ€çµ‚çµ±åˆå“è³ªä¿è¨¼---ãƒ†ã‚¹ãƒˆä»•æ§˜tdd)
   - [çµ±åˆãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰](#çµ±åˆãƒ†ã‚¹ãƒˆä»•æ§˜tdd)
   - [ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ä»•æ§˜](#ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ä»•æ§˜)
   - [ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæˆ¦ç•¥](#ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæˆ¦ç•¥)
7. [æˆæœç‰©](#7-æˆæœç‰©)
8. [Phase 8æˆåŠŸåŸºæº–](#8-phase-8æˆåŠŸåŸºæº–)

---

## 1. Phase 8æ¦‚è¦

### 1.1 ç›®çš„

LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é€²åŒ–ã¨å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã«ã‚ˆã‚Šã€**å®Œå…¨ãªä¼šè©±AIã‚·ã‚¹ãƒ†ãƒ **ã‚’å®Œæˆã•ã›ã¾ã™ã€‚

### 1.2 TDDå®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

Phase 8ã¯**ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼ˆTDDï¼‰**ã§å®Ÿè£…ã—ã¾ã™ã€‚å„æ©Ÿèƒ½ã¯ä»¥ä¸‹ã®ã‚µã‚¤ã‚¯ãƒ«ã§é–‹ç™ºã—ã¾ã™ï¼š

```
1. ğŸ”´ RED: ãƒ†ã‚¹ãƒˆã‚’æ›¸ãï¼ˆå¤±æ•—ã™ã‚‹ï¼‰
2. ğŸŸ¢ GREEN: æœ€å°é™ã®å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã‚’é€šã™
3. ğŸ”µ REFACTOR: ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ†ã‚¹ãƒˆã¯å¸¸ã«æˆåŠŸï¼‰
```

**TDDã®åŸå‰‡**:
- âœ… å®Ÿè£…å‰ã«å¿…ãšãƒ†ã‚¹ãƒˆã‚’æ›¸ã
- âœ… 1ã¤ã®ãƒ†ã‚¹ãƒˆ â†’ 1ã¤ã®å®Ÿè£… â†’ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®ã‚µã‚¤ã‚¯ãƒ«
- âœ… Given-When-Thenå½¢å¼ã§ãƒ†ã‚¹ãƒˆã‚’è¨˜è¿°
- âœ… å„ãƒ†ã‚¹ãƒˆã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½
- âœ… å¤–éƒ¨ä¾å­˜ã¯ãƒ¢ãƒƒã‚¯ã§åˆ†é›¢ï¼ˆGPUä¸è¦ãªè»½é‡ãƒ†ã‚¹ãƒˆã‚’å„ªå…ˆï¼‰

### 1.3 ä¸»è¦æ©Ÿèƒ½

| æ©Ÿèƒ½ã‚«ãƒ†ã‚´ãƒª | èª¬æ˜ | Priority |
|-------------|------|----------|
| **LoRAé©ç”¨** | æœˆæ¬¡ãƒãƒƒãƒå‡¦ç† | ğŸŸ¡ Medium |
| **çµ±åˆãƒ†ã‚¹ãƒˆ** | Phase 1-7æ¨ªæ–­ãƒ†ã‚¹ãƒˆ | ğŸ”´ High |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–** | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ”¹å–„ | ğŸ”´ High |
| **ãƒªãƒªãƒ¼ã‚¹æº–å‚™** | ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† | ğŸ”´ High |

### 1.3 é”æˆç›®æ¨™

âœ… LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ä½œç¢ºèª  
âœ… å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ  
âœ… ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†  
âœ… v4.0.0ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆä½œæˆ

---

## 2. å‰ææ¡ä»¶

### 2.1 Phase 1-7å®Œäº†äº‹é …

âœ… **Phase 1**: LangGraphã‚³ã‚¢ãƒ»5éšå±¤è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ   
âœ… **Phase 2**: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£  
âœ… **Phase 3**: REST/WebSocket APIï¼ˆ23ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼‰  
âœ… **Phase 4**: é€£æƒ³è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãƒ»æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«åŸºç›¤  
âœ… **Phase 5**: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œãƒ»è‡ªå·±çœå¯Ÿ  
âœ… **Phase 6**: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ãƒ»MCP Server  
âœ… **Phase 7**: 3Då¯è¦–åŒ–ãƒ»è‡ªå¾‹ã‚µãƒ¼ãƒ

**å‚ç…§**: [`docks/å®Ÿè£…ä»•æ§˜/Phase7_å®Ÿè£…ä»•æ§˜.md`](Phase7_å®Ÿè£…ä»•æ§˜.md:1)

### 2.2 ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

**LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨**:
- **GPU**: VRAM 8GBä»¥ä¸Šï¼ˆNVIDIAæ¨å¥¨ï¼‰
- **CPUä»£æ›¿**: å¯èƒ½ã ãŒ10å€ä»¥ä¸Šã®æ™‚é–“
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Š

---

## 3. Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

### 3.1 å®Ÿè£…å†…å®¹

**å‚ç…§**: [`docks/ä»•æ§˜æ›¸/03_ä¼šè©±LLM_ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»•æ§˜.md:335-379`](../ä»•æ§˜æ›¸/03_ä¼šè©±LLM_ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä»•æ§˜.md:335)

#### 3.1.1 æœˆæ¬¡ãƒãƒƒãƒå‡¦ç†

**å‡¦ç†ãƒ•ãƒ­ãƒ¼**:
1. **ä¼šè©±å±¥æ­´åé›†**: éå»1ãƒ¶æœˆåˆ†ã®å…¨ä¼šè©±
2. **è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ**: Alpacaå½¢å¼å¤‰æ›
3. **LoRAé©ç”¨**: PEFTãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§å­¦ç¿’
4. **ãƒ¢ãƒ‡ãƒ«ä¿å­˜**: `models/lora_{character_name}/`

#### 3.1.2 è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢å¼

```json
{
  "instruction": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚„æŒ‡ç¤º",
  "input": "è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
  "output": "ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å¿œç­”"
}
```

**ä¾‹**:
```json
{
  "instruction": "æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
  "input": "",
  "output": "æ©Ÿæ¢°å­¦ç¿’ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä¸»ã«æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ã®3ã¤ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚"
}
```

### 3.2 ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

#### training/lora_tuning.py (400è¡Œ)

```python
"""LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«."""

from typing import List, Dict, Any
import os
import json
from datetime import datetime, timedelta
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

class CharacterFineTuning:
    """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹."""
    
    def __init__(
        self,
        base_model: str = "gpt2",
        lora_r: int = 8,
        lora_alpha: int = 16,
        lora_dropout: float = 0.1
    ):
        """
        åˆæœŸåŒ–.
        
        Args:
            base_model: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å
            lora_r: LoRAãƒ©ãƒ³ã‚¯
            lora_alpha: LoRAã‚¢ãƒ«ãƒ•ã‚¡
            lora_dropout: LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
        """
        self.base_model_name = base_model
        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["c_attn"],
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
    
    def fine_tune_character(
        self,
        character_name: str,
        num_epochs: int = 3,
        batch_size: int = 4
    ) -> str:
        """
        ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        Returns:
            str: ä¿å­˜ãƒ‘ã‚¹
        """
        # 1. ä¼šè©±å±¥æ­´åé›†
        conversations = self._collect_conversations(character_name, days=30)
        
        if len(conversations) < 100:
            raise ValueError(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(conversations)}ä»¶ï¼ˆæœ€ä½100ä»¶å¿…è¦ï¼‰")
        
        # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        training_data = self._create_training_data(conversations)
        
        # 3. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
        model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4. LoRAé©ç”¨
        model = get_peft_model(model, self.lora_config)
        
        # 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = Dataset.from_list(training_data)
        
        def tokenize_function(examples):
            prompts = [
                f"Instruction: {inst}\nInput: {inp}\nOutput: {out}"
                for inst, inp, out in zip(
                    examples["instruction"],
                    examples["input"],
                    examples["output"]
                )
            ]
            return tokenizer(
                prompts,
                truncation=True,
                padding="max_length",
                max_length=512
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # 6. è¨“ç·´è¨­å®š
        output_dir = f"models/lora_{character_name}"
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            save_steps=100,
            logging_steps=10,
            learning_rate=3e-4,
            fp16=torch.cuda.is_available(),
        )
        
        # 7. è¨“ç·´å®Ÿè¡Œï¼ˆå®Ÿéš›ã¯Trainerã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰
        # trainer = Trainer(...)
        # trainer.train()
        
        # 8. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        return output_dir
    
    def _collect_conversations(
        self,
        character_name: str,
        days: int
    ) -> List[Dict[str, Any]]:
        """
        ä¼šè©±å±¥æ­´åé›†.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            days: åé›†æ—¥æ•°
        
        Returns:
            List[Dict]: ä¼šè©±å±¥æ­´
        """
        from services.chat_service import ChatService
        
        chat_service = ChatService()
        
        # éå»Næ—¥åˆ†ã®ä¼šè©±å–å¾—
        start_date = datetime.utcnow() - timedelta(days=days)
        
        conversations = chat_service.get_history(
            character_name=character_name,
            start_date=start_date,
            limit=10000
        )
        
        return conversations
    
    def _create_training_data(
        self,
        conversations: List[Dict[str, Any]]
    ) -> List[Dict[str, str]]:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆAlpacaå½¢å¼ï¼‰.
        
        Args:
            conversations: ä¼šè©±å±¥æ­´
        
        Returns:
            List[Dict]: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        """
        training_data = []
        
        for conv in conversations:
            # user â†’ assistant ã®ãƒšã‚¢æŠ½å‡º
            if conv.get("role") == "user":
                user_input = conv.get("content", "")
                
                # æ¬¡ã®assistantå¿œç­”ã‚’æ¢ã™
                assistant_output = None
                for i, c in enumerate(conversations):
                    if c.get("timestamp") > conv.get("timestamp") and c.get("role") == "assistant":
                        assistant_output = c.get("content")
                        break
                
                if assistant_output:
                    training_data.append({
                        "instruction": user_input,
                        "input": "",
                        "output": assistant_output
                    })
        
        return training_data
    
    def load_lora_model(self, character_name: str):
        """
        LoRAãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰.
        
        Args:
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
        
        Returns:
            Model: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        """
        from peft import PeftModel
        
        model_path = f"models/lora_{character_name}"
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"LoRAãƒ¢ãƒ‡ãƒ«ãªã—: {model_path}")
        
        base_model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
        model = PeftModel.from_pretrained(base_model, model_path)
        
        return model
```

### 3.3 ãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰

**æ³¨æ„**: ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å®Ÿè£…å‰ã®ãƒ†ã‚¹ãƒˆä»•æ§˜ã§ã™ã€‚å®Ÿè£…ã¯å¿…ãšãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã§è¡Œã„ã¾ã™ã€‚

**é‡è¦**: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯GPUã‚’å¿…è¦ã¨ã™ã‚‹ãŸã‚ã€**GPUä¸è¦ãªè»½é‡ãƒ†ã‚¹ãƒˆã‚’å„ªå…ˆ**ã—ã€GPUãƒ†ã‚¹ãƒˆã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¨ã—ã¦å®Ÿè£…ã—ã¾ã™ã€‚

#### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

- `tests/test_lora_tuning.py`: CharacterFineTuningã‚¯ãƒ©ã‚¹ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆï¼ˆ30ä»¶ã€GPUä¸è¦ï¼‰
- `tests/test_integration_lora.py`: LoRAçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ10ä»¶ã€GPUä¸è¦ï¼‰

#### ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å®šç¾©

```python
# tests/fixtures/lora_tuning_fixtures.py

TEST_CHARACTERS = ["lumina", "clarisse", "nox"]
TEST_CONVERSATIONS = [
    {
        "role": "user",
        "content": "ã“ã‚“ã«ã¡ã¯",
        "timestamp": "2025-11-20T10:00:00Z"
    },
    {
        "role": "assistant",
        "content": "ã“ã‚“ã«ã¡ã¯ï¼",
        "timestamp": "2025-11-20T10:00:01Z"
    }
]

TEST_LORA_CONFIGS = [
    {"r": 8, "alpha": 16, "dropout": 0.1},
    {"r": 16, "alpha": 32, "dropout": 0.05},
    {"r": 4, "alpha": 8, "dropout": 0.2},
]

TEST_EPOCHS = [1, 3, 5, 10]
TEST_BATCH_SIZES = [1, 4, 8, 16]
```

---

## 4. Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼

### 4.1 å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆTDDï¼‰

**æ³¨æ„**: ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å®Ÿè£…å‰ã®ãƒ†ã‚¹ãƒˆä»•æ§˜ã§ã™ã€‚å®Ÿè£…ã¯å¿…ãšãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã§è¡Œã„ã¾ã™ã€‚

#### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

- `tests/test_integration_phase1_7.py`: Phase 1-7æ¨ªæ–­çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ20ä»¶ï¼‰
- `tests/test_integration_phase4_8.py`: Phase 4-8çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ15ä»¶ï¼‰
- `tests/test_e2e.py`: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆ10ä»¶ï¼‰
- `tests/test_performance.py`: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

---

## Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼ - ãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: `tests/test_integration_phase1_7.py`

**ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹**: `TestPhase1_7Integration`

**ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ20ä»¶ï¼‰**:

#### 1. LangGraphã‚³ã‚¢çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_langgraph_conversation_flow():
    """
    Given: LangGraphã‚³ã‚¢ã¨ä¼šè©±ã‚µãƒ¼ãƒ“ã‚¹
    When: ä¼šè©±ã‚’å®Ÿè¡Œ
    Then: LangGraphãƒ•ãƒ­ãƒ¼ãŒæ­£ã—ãå‹•ä½œã™ã‚‹
    """
    from services.chat_service import ChatService
    
    chat_service = ChatService()
    
    response = await chat_service.chat(
        user_input="ã“ã‚“ã«ã¡ã¯",
        character_name="lumina",
        session_id="test_001"
    )
    
    assert response["status"] == "success"
    assert len(response["response"]) > 0

@pytest.mark.integration
def test_memory_hierarchy_integration():
    """
    Given: 5éšå±¤è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ 
    When: è¨˜æ†¶ã‚’ä¿å­˜ãƒ»æ¤œç´¢
    Then: å„éšå±¤ãŒæ­£ã—ãå‹•ä½œã™ã‚‹
    """
    from memory.working import WorkingMemory
    from memory.short_term import ShortTermMemory
    from memory.long_term import LongTermMemory
    
    # å„éšå±¤ã®ãƒ†ã‚¹ãƒˆ
    working = WorkingMemory()
    working.store("ãƒ†ã‚¹ãƒˆè¨˜æ†¶")
    
    assert len(working.retrieve()) > 0
```

#### 2. é€£æƒ³è¨˜æ†¶ãƒ»æ„Ÿæƒ…çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

```python
@pytest.mark.integration
def test_associative_memory_with_emotion():
    """
    Given: é€£æƒ³è¨˜æ†¶ã¨æ„Ÿæƒ…ã‚·ã‚¹ãƒ†ãƒ 
    When: æ¦‚å¿µã‚’è¿½åŠ ã—ã€æ„Ÿæƒ…ã‚’æ›´æ–°
    Then: é€£æƒ³è¨˜æ†¶ã¨æ„Ÿæƒ…ãŒé€£æºã™ã‚‹
    """
    from memory.associative import AssociativeMemory
    from core.emotion import EmotionalState
    
    memory = AssociativeMemory(db_path=":memory:")
    emotion = EmotionalState("lumina")
    
    memory.add_concept("Python", embedding=[0.1]*128, metadata={})
    emotion.update_emotion("joy", intensity=0.8)
    
    # é€£æƒ³æ¤œç´¢
    results = memory.retrieve_associated_concepts("Python", depth=2, threshold=0.5)
    
    assert len(results) >= 0
```

#### 3. å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»è‡ªå·±çœå¯Ÿçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

```python
@pytest.mark.integration
def test_dialogue_style_with_reflection():
    """
    Given: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œã¨è‡ªå·±çœå¯Ÿ
    When: ä¼šè©±å¾Œã«çœå¯Ÿã‚’å®Ÿè¡Œ
    Then: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œã‚‹
    """
    from core.dialogue_style import AdaptiveDialogueStyle
    from core.self_reflection import SelfReflection
    
    style = AdaptiveDialogueStyle("user_001")
    reflection = SelfReflection("lumina")
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å­¦ç¿’
    style.learn_from_feedback({
        "message": "ã‚‚ã£ã¨è©³ã—ãèª¬æ˜ã—ã¦ã»ã—ã„",
        "thumbs_up": True
    })
    
    # çœå¯Ÿ
    conversation = [
        {"role": "user", "content": "ã‚ã‹ã‚Šã‚„ã™ã", "session_id": "sess_001"}
    ]
    result = reflection.reflect_on_conversation(conversation)
    
    assert result is not None
    assert style.parameters["verbosity"] > 0.5
```

#### 4. ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ãƒ»MCPçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

```python
@pytest.mark.integration
def test_character_growth_with_mcp():
    """
    Given: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ã¨MCP Server
    When: KPIã‚’æ›´æ–°ã—ã€MCPçµŒç”±ã§æƒ…å ±å–å¾—
    Then: æˆé•·æƒ…å ±ãŒMCPçµŒç”±ã§å–å¾—ã§ãã‚‹
    """
    from core.character_growth import CharacterGrowth
    from api.mcp_server import LlmMultiChatMCPServer
    
    growth = CharacterGrowth("lumina", db_path=":memory:")
    
    # KPIæ›´æ–°
    for _ in range(10):
        growth.update_kpi("user_thumbs_up", value=1)
    
    # MCPçµŒç”±ã§æƒ…å ±å–å¾—
    server = LlmMultiChatMCPServer()
    
    import asyncio
    info = asyncio.run(server._get_character_info("lumina"))
    
    assert "lumina" in info.lower()
    assert growth.current_level == 1
```

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: `tests/test_integration_phase4_8.py`

**ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹**: `TestPhase4_8Integration`

**ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ15ä»¶ï¼‰**:

```python
"""Phase 4-8çµ±åˆãƒ†ã‚¹ãƒˆ."""

import pytest
from services.chat_service import ChatService
from memory.associative import AssociativeMemory
from core.emotion import EmotionalState
from core.dialogue_style import AdaptiveDialogueStyle
from core.character_growth import CharacterGrowth
from api.mcp_server import LlmMultiChatMCPServer
from visualization.association_3d import AssociationVisualizationPanel
from agents.autonomous_search import AutonomousSearchAgent


@pytest.mark.integration
@pytest.mark.asyncio
async def test_end_to_end_conversation():
    """
    Given: å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
    When: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ä¼šè©±ã‚’å®Ÿè¡Œ
    Then: ã™ã¹ã¦ã®ã‚·ã‚¹ãƒ†ãƒ ãŒé€£æºã—ã¦å‹•ä½œã™ã‚‹
    """
    chat_service = ChatService()
    
    response = await chat_service.chat(
        user_input="æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        character_name="lumina",
        session_id="integration_test_001"
    )
    
    assert response["status"] == "success"
    assert len(response["response"]) > 0

@pytest.mark.integration
def test_associative_memory_integration():
    """
    Given: é€£æƒ³è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ 
    When: æ¦‚å¿µã‚’è¿½åŠ ãƒ»æ¤œç´¢
    Then: é€£æƒ³è¨˜æ†¶ãŒæ­£ã—ãå‹•ä½œã™ã‚‹
    """
    memory = AssociativeMemory(db_path=":memory:")
    
    memory.add_concept("Python", embedding=[0.1]*128, metadata={})
    memory.add_concept("æ©Ÿæ¢°å­¦ç¿’", embedding=[0.2]*128, metadata={})
    memory.link_concepts("Python", "æ©Ÿæ¢°å­¦ç¿’", "related", strength=0.8)
    
    results = memory.retrieve_associated_concepts("Python", depth=2, threshold=0.5)
    
    assert len(results) > 0

@pytest.mark.integration
def test_character_growth_with_kpi():
    """
    Given: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ã‚·ã‚¹ãƒ†ãƒ 
    When: KPIã‚’æ›´æ–°
    Then: ãƒ¬ãƒ™ãƒ«ã‚¢ãƒƒãƒ—ãŒç™ºç”Ÿã™ã‚‹
    """
    growth = CharacterGrowth("lumina", db_path=":memory:")
    
    for _ in range(10):
        growth.update_kpi("user_thumbs_up", value=1)
    
    assert growth.current_level == 1

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mcp_server_integration():
    """
    Given: MCP Server
    When: ãƒãƒ£ãƒƒãƒˆãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™
    Then: ãƒãƒ£ãƒƒãƒˆå¿œç­”ãŒè¿”ã•ã‚Œã‚‹
    """
    server = LlmMultiChatMCPServer()
    
    result = await server._chat_with_character("lumina", "ã“ã‚“ã«ã¡ã¯")
    
    assert len(result) > 0

@pytest.mark.integration
def test_visualization_with_associative_memory():
    """
    Given: 3Då¯è¦–åŒ–ã¨é€£æƒ³è¨˜æ†¶
    When: ã‚°ãƒ©ãƒ•ã‚’æç”»
    Then: é€£æƒ³è¨˜æ†¶ã‹ã‚‰æ¦‚å¿µãŒå–å¾—ã•ã‚Œã€ã‚°ãƒ©ãƒ•ãŒæç”»ã•ã‚Œã‚‹
    """
    memory = AssociativeMemory(db_path=":memory:")
    panel = AssociationVisualizationPanel(memory)
    
    memory.add_concept("æ©Ÿæ¢°å­¦ç¿’", embedding=[0.1]*128, metadata={})
    panel.current_center = "æ©Ÿæ¢°å­¦ç¿’"
    
    fig = panel._render_graph()
    
    assert fig is not None

@pytest.mark.integration
def test_autonomous_search_with_kb():
    """
    Given: è‡ªå¾‹ã‚µãƒ¼ãƒã¨KB
    When: æ¤œç´¢ã—ã¦KBã«ä¿å­˜
    Then: KBã«æ­£ã—ãä¿å­˜ã•ã‚Œã‚‹
    """
    agent = AutonomousSearchAgent()
    
    with patch.object(agent, 'web_search', return_value=[
        {"title": "çµæœ", "snippet": "èª¬æ˜", "link": "http://example.com"}
    ]):
        agent.save_to_kb("ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "news")
        
        # KBã‹ã‚‰æ¤œç´¢
        results = agent.kb.search("ãƒ†ã‚¹ãƒˆ", top_k=1)
        assert len(results) >= 0

# ... ä»–9ä»¶ï¼ˆå…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã€ã‚¨ãƒ©ãƒ¼å›å¾©ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãªã©ï¼‰
```

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: `tests/test_e2e.py`

**ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹**: `TestEndToEnd`

**ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ10ä»¶ï¼‰**:

```python
"""ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ."""

import pytest
from services.chat_service import ChatService


@pytest.mark.e2e
@pytest.mark.asyncio
async def test_complete_conversation_flow():
    """
    Given: å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
    When: å®Œå…¨ãªä¼šè©±ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
    Then: ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒé€£æºã—ã¦å‹•ä½œã™ã‚‹
    """
    chat_service = ChatService()
    
    # 1. ä¼šè©±é–‹å§‹
    response1 = await chat_service.chat(
        user_input="ã“ã‚“ã«ã¡ã¯",
        character_name="lumina",
        session_id="e2e_test_001"
    )
    assert response1["status"] == "success"
    
    # 2. ç¶šãã®ä¼šè©±
    response2 = await chat_service.chat(
        user_input="æ©Ÿæ¢°å­¦ç¿’ã«ã¤ã„ã¦æ•™ãˆã¦",
        character_name="lumina",
        session_id="e2e_test_001"
    )
    assert response2["status"] == "success"
    
    # 3. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    # å®Ÿè£…ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_multi_character_conversation():
    """
    Given: è¤‡æ•°ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼
    When: å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ä¼šè©±
    Then: å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹
    """
    chat_service = ChatService()
    
    characters = ["lumina", "clarisse", "nox"]
    
    for char in characters:
        response = await chat_service.chat(
            user_input="ã“ã‚“ã«ã¡ã¯",
            character_name=char,
            session_id=f"e2e_test_{char}"
        )
        assert response["status"] == "success"

# ... ä»–8ä»¶ï¼ˆé•·æ™‚é–“ä¼šè©±ã€å¤§é‡ãƒ‡ãƒ¼ã‚¿ã€ã‚¨ãƒ©ãƒ¼å›å¾©ãªã©ï¼‰
```

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: `tests/test_performance.py`

**ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹**: `TestPerformance`

**ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ5ä»¶ï¼‰**:

```python
"""ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ."""

import pytest
import time
from services.chat_service import ChatService


@pytest.mark.performance
@pytest.mark.asyncio
async def test_chat_response_time():
    """
    Given: ChatService
    When: ä¼šè©±ã‚’å®Ÿè¡Œ
    Then: å¿œç­”æ™‚é–“ãŒè¨±å®¹ç¯„å›²å†…ï¼ˆ< 5ç§’ï¼‰
    """
    chat_service = ChatService()
    
    start_time = time.time()
    response = await chat_service.chat(
        user_input="ãƒ†ã‚¹ãƒˆ",
        character_name="lumina",
        session_id="perf_test_001"
    )
    elapsed_time = time.time() - start_time
    
    assert response["status"] == "success"
    assert elapsed_time < 5.0

@pytest.mark.performance
def test_memory_search_performance():
    """
    Given: å¤§é‡ã®è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿
    When: æ¤œç´¢ã‚’å®Ÿè¡Œ
    Then: æ¤œç´¢æ™‚é–“ãŒè¨±å®¹ç¯„å›²å†…ï¼ˆ< 1ç§’ï¼‰
    """
    from memory.long_term import LongTermMemory
    
    memory = LongTermMemory()
    
    # å¤§é‡ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
    # æ¤œç´¢æ™‚é–“æ¸¬å®š
    
    start_time = time.time()
    results = memory.search("ãƒ†ã‚¹ãƒˆ", top_k=10)
    elapsed_time = time.time() - start_time
    
    assert elapsed_time < 1.0

# ... ä»–3ä»¶ï¼ˆä¸¦è¡Œå‡¦ç†ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€CPUä½¿ç”¨ç‡ãªã©ï¼‰
```

---

## çµ±åˆãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé †åº

1. **Phase 1-7çµ±åˆãƒ†ã‚¹ãƒˆ**: å„Phaseã®æ©Ÿèƒ½ãŒæ­£ã—ãé€£æºã—ã¦ã„ã‚‹ã‹ç¢ºèª
2. **Phase 4-8çµ±åˆãƒ†ã‚¹ãƒˆ**: æ–°æ©Ÿèƒ½ï¼ˆPhase 4-8ï¼‰ã®çµ±åˆç¢ºèª
3. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ**: å®Œå…¨ãªä¼šè©±ãƒ•ãƒ­ãƒ¼ã®ç¢ºèª
4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ**: å¿œç­”æ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç¢ºèª

---

## ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ä»•æ§˜

### conftest.py ã®æ‹¡å¼µ

```python
# tests/conftest.pyï¼ˆæ‹¡å¼µï¼‰

import pytest
import tempfile
from unittest.mock import Mock, patch, AsyncMock

from training.lora_tuning import CharacterFineTuning
from services.chat_service import ChatService
from memory.associative import AssociativeMemory
from core.character_growth import CharacterGrowth
from api.mcp_server import LlmMultiChatMCPServer

@pytest.fixture
def character_fine_tuning():
    """CharacterFineTuningã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
    return CharacterFineTuning()

@pytest.fixture
def mock_chat_service():
    """ChatServiceã®ãƒ¢ãƒƒã‚¯"""
    mock = AsyncMock()
    mock.chat = AsyncMock(return_value={
        "status": "success",
        "response": "ãƒ¢ãƒƒã‚¯å¿œç­”"
    })
    mock.get_history = Mock(return_value=[])
    return mock

@pytest.fixture
def mock_transformer_model():
    """Transformersãƒ¢ãƒ‡ãƒ«ã®ãƒ¢ãƒƒã‚¯"""
    with patch('transformers.AutoModelForCausalLM.from_pretrained') as mock_model, \
         patch('transformers.AutoTokenizer.from_pretrained') as mock_tokenizer:
        mock_model.return_value = Mock()
        mock_tokenizer.return_value = Mock(pad_token=None, eos_token="<eos>")
        yield mock_model, mock_tokenizer

@pytest.fixture
def temp_model_dir():
    """ä¸€æ™‚çš„ãªãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    import tempfile
    import os
    
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    
    import shutil
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
```

---

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæˆ¦ç•¥

### TDDå®Ÿè£…é †åºï¼ˆè©³ç´°ç‰ˆï¼‰

#### Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**Day 1: åˆæœŸåŒ–ãƒ»è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆï¼ˆ15ä»¶ï¼‰â†’ å®Ÿè£…**
- åˆæœŸåŒ–: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã€ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã€LoRAè¨­å®š
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ: åŸºæœ¬å¤‰æ›ã€è¤‡æ•°ãƒšã‚¢ã€ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹

**Day 2: ä¼šè©±å±¥æ­´åé›†ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆ15ä»¶ï¼‰â†’ å®Ÿè£…**
- ä¼šè©±å±¥æ­´åé›†: åŸºæœ¬åé›†ã€ç©ºçµæœã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€ååˆ†ãªãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒƒã‚¯ä½¿ç”¨

**Day 3: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆ15ä»¶ï¼‰â†’ å®Ÿè£…**
- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: å­˜åœ¨ç¢ºèªã€ãƒ­ãƒ¼ãƒ‰æˆåŠŸã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹: æ¬ æãƒ‡ãƒ¼ã‚¿ã€ç•°å¸¸å€¤ã€å¢ƒç•Œå€¤

**Day 4-5: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ï¼ˆ5ä»¶ï¼‰â†’ å®Ÿè£…ãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**

#### Week 3: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼

**Day 1-2: Phase 1-7çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ20ä»¶ï¼‰â†’ å®Ÿè£…**
- LangGraphã‚³ã‚¢çµ±åˆã€è¨˜æ†¶éšå±¤çµ±åˆã€é€£æƒ³è¨˜æ†¶ãƒ»æ„Ÿæƒ…çµ±åˆ

**Day 3: Phase 4-8çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆ15ä»¶ï¼‰â†’ å®Ÿè£…**
- å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»è‡ªå·±çœå¯Ÿçµ±åˆã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ãƒ»MCPçµ±åˆ

**Day 4: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆ10ä»¶ï¼‰â†’ å®Ÿè£…**
- å®Œå…¨ãªä¼šè©±ãƒ•ãƒ­ãƒ¼ã€è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€é•·æ™‚é–“ä¼šè©±

**Day 5: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰â†’ å®Ÿè£…ãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°**
- å¿œç­”æ™‚é–“ã€æ¤œç´¢æ™‚é–“ã€ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡

### ãƒ†ã‚¹ãƒˆå“è³ªåŸºæº–

**å¿…é ˆè¦ä»¶**:
- âœ… **ãƒ†ã‚¹ãƒˆæˆåŠŸç‡**: 100%ï¼ˆå…¨95ä»¶ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸï¼‰
- âœ… **ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸**: 83%ä»¥ä¸Šï¼ˆå¹³å‡ï¼‰
- âœ… **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“**: å…¨ãƒ†ã‚¹ãƒˆ10åˆ†ä»¥å†…ï¼ˆGPUãƒ†ã‚¹ãƒˆé™¤ãï¼‰
- âœ… **ãƒ†ã‚¹ãƒˆç‹¬ç«‹æ€§**: å„ãƒ†ã‚¹ãƒˆã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½
- âœ… **ãƒ¢ãƒƒã‚¯ä½¿ç”¨**: å¤–éƒ¨ä¾å­˜ï¼ˆGPUã€ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ç­‰ï¼‰ã¯ãƒ¢ãƒƒã‚¯ã§åˆ†é›¢

**TDDã‚µã‚¤ã‚¯ãƒ«éµå®ˆ**:
- âœ… **RED**: å®Ÿè£…å‰ã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ã„ã¦ã„ã‚‹
- âœ… **GREEN**: æœ€å°é™ã®å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã‚’é€šã—ã¦ã„ã‚‹
- âœ… **REFACTOR**: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã‚‚ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¦ã„ã‚‹

---

## 7. æˆæœç‰©

### 4.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### profiler/performance_analysis.py

```python
"""ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ."""

import time
import cProfile
import pstats
from io import StringIO


def profile_chat_service():
    """ChatServiceãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«."""
    profiler = cProfile.Profile()
    
    profiler.enable()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    from services.chat_service import ChatService
    chat_service = ChatService()
    
    for _ in range(10):
        chat_service.chat(
            user_input="ãƒ†ã‚¹ãƒˆ",
            character_name="lumina",
            session_id="bench_001"
        )
    
    profiler.disable()
    
    # çµæœå‡ºåŠ›
    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    ps.print_stats(10)
    
    print(s.getvalue())
```

### 4.3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ€çµ‚æ›´æ–°

**ä½œæˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**:
1. **å®Œå…¨ä»•æ§˜æ›¸æ›´æ–°**: å…¨Phaseçµ±åˆå†…å®¹
2. **APIä»•æ§˜æ›¸æœ€çµ‚ç‰ˆ**: å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§
3. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰**: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ä½¿ç”¨æ–¹æ³•
4. **ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0**: å¤‰æ›´å±¥æ­´

### 4.4 ãƒªãƒªãƒ¼ã‚¹æº–å‚™

#### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0

```markdown
# LlmMultiChat3 v4.0.0 ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ

**ãƒªãƒªãƒ¼ã‚¹æ—¥**: 2025å¹´XXæœˆXXæ—¥

## ğŸ‰ æ–°æ©Ÿèƒ½

### Phase 4: è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ + æ„Ÿæƒ…åŸºç›¤
- é€£æƒ³è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSQLite Graphï¼‰
- 8åŸºæœ¬æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«ï¼ˆPlutchikï¼‰

### Phase 5: å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«é©å¿œ + è‡ªå·±çœå¯Ÿ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«è‡ªå‹•èª¿æ•´
- è‡ªå·±çœå¯Ÿãƒ»çŸ›ç›¾æ¤œå‡º

### Phase 6: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•· + MCPå¯¾å¿œ
- KPIãƒ™ãƒ¼ã‚¹æˆé•·ã‚·ã‚¹ãƒ†ãƒ 
- MCP Serverå®Ÿè£…ï¼ˆClaude Desktopçµ±åˆï¼‰

### Phase 7: 3Då¯è¦–åŒ– + è‡ªå¾‹ã‚µãƒ¼ãƒ
- é€£æƒ³ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯3Då¯è¦–åŒ–ï¼ˆPlotly.jsï¼‰
- è‡ªå¾‹çš„å¤–éƒ¨ã‚µãƒ¼ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

### Phase 8: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æœ€çµ‚çµ±åˆ
- æœˆæ¬¡ãƒãƒƒãƒLoRAå­¦ç¿’
- å…¨Phaseçµ±åˆå®Œäº†

## ğŸ“Š æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

- **Backend**: Python 3.9+, FastAPI, LangGraph
- **Frontend**: React 18, TypeScript, Vite
- **Database**: SQLite, Redis
- **AI**: OpenAI API, Ollama
- **Visualization**: Plotly.js

## ğŸš€ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
git clone https://github.com/Nyukimin/LlmMultiChat3.git
cd LlmMultiChat3
pip install -r requirements.txt
uvicorn main:app --reload
```

## ğŸ“– ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [å®Œå…¨ä»•æ§˜æ›¸](docks/ä»•æ§˜æ›¸/)
- [APIä»•æ§˜æ›¸](docks/APIä»•æ§˜æ›¸.md)
- [ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰](docks/ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰.md)

## ğŸ› æ—¢çŸ¥ã®å•é¡Œ

- LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯GPUæ¨å¥¨ï¼ˆCPUç‰ˆã¯ä½é€Ÿï¼‰

## ğŸ‘ è²¢çŒ®è€…

- @Nyukimin
```

---

## 5. æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### 5.1 Pythonä¾å­˜

```txt
# requirements.txt ã«è¿½åŠ 
transformers==4.35.0    # LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
peft==0.6.0             # PEFTï¼ˆLoRAï¼‰
torch==2.1.0            # PyTorch
datasets==2.15.0        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
```

### 5.2 æ–°è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

- **training/lora_tuning.py**: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

---

## 6. ãƒ†ã‚¹ãƒˆè¨ˆç”»ï¼ˆTDDå®Ÿè£…ï¼‰

### 6.0 TDDå®Ÿè£…ä»•æ§˜ã‚µãƒãƒªãƒ¼

**Phase 8ã¯å®Œå…¨ãªTDDï¼ˆãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å®Ÿè£…ã—ã¾ã™ã€‚**

#### TDDå®Ÿè£…ã®åŸå‰‡

1. **ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**: ã™ã¹ã¦ã®æ©Ÿèƒ½ã¯å®Ÿè£…å‰ã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
2. **RED-GREEN-REFACTORã‚µã‚¤ã‚¯ãƒ«**: å¤±æ•—â†’æˆåŠŸâ†’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’å¾¹åº•
3. **Given-When-Thenå½¢å¼**: ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’æ˜ç¢ºãªå½¢å¼ã§è¨˜è¿°
4. **ãƒ†ã‚¹ãƒˆç‹¬ç«‹æ€§**: å„ãƒ†ã‚¹ãƒˆã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½
5. **ãƒ¢ãƒƒã‚¯åˆ†é›¢**: å¤–éƒ¨ä¾å­˜ï¼ˆGPUã€ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ç­‰ï¼‰ã¯ãƒ¢ãƒƒã‚¯ã§åˆ†é›¢
6. **è»½é‡ãƒ†ã‚¹ãƒˆå„ªå…ˆ**: GPUä¸è¦ãªãƒ†ã‚¹ãƒˆã‚’å„ªå…ˆã—ã€GPUãƒ†ã‚¹ãƒˆã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³

#### ãƒ†ã‚¹ãƒˆæ§‹æˆ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« | ãƒ†ã‚¹ãƒˆæ•° | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ | å„ªå…ˆåº¦ |
|---------|--------------|---------|--------------|--------|
| **LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** |
| CharacterFineTuning | `test_lora_tuning.py` | 30ä»¶ + ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹10ä»¶ + ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–5ä»¶ | 85%ä»¥ä¸Š | ğŸŸ¡ Medium |
| **æœ€çµ‚çµ±åˆãƒ†ã‚¹ãƒˆ** |
| Phase 1-7çµ±åˆ | `test_integration_phase1_7.py` | 20ä»¶ | 85%ä»¥ä¸Š | ğŸ”´ High |
| Phase 4-8çµ±åˆ | `test_integration_phase4_8.py` | 15ä»¶ | 85%ä»¥ä¸Š | ğŸ”´ High |
| ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ | `test_e2e.py` | 10ä»¶ | 80%ä»¥ä¸Š | ğŸ”´ High |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ** |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ | `test_performance.py` | 5ä»¶ | 70%ä»¥ä¸Š | ğŸŸ¡ Medium |
| **åˆè¨ˆ** | **5ãƒ•ã‚¡ã‚¤ãƒ« + ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£1ãƒ•ã‚¡ã‚¤ãƒ«** | **105ä»¶ä»¥ä¸Š** | **å¹³å‡83%ä»¥ä¸Š** | - |

#### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæˆ¦ç•¥

- **Week 1-2**: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ5æ—¥é–“ã§æ®µéšçš„ã«å®Ÿè£…ã€GPUä¸è¦ãƒ†ã‚¹ãƒˆå„ªå…ˆï¼‰
- **Week 3**: æœ€çµ‚çµ±åˆãƒ»å“è³ªä¿è¨¼ï¼ˆ5æ—¥é–“ã§æ®µéšçš„ã«å®Ÿè£…ï¼‰
- **å„æ©Ÿèƒ½**: RED â†’ GREEN â†’ REFACTORã‚µã‚¤ã‚¯ãƒ«ã§å®Ÿè£…
- **å“è³ªåŸºæº–**: ãƒ†ã‚¹ãƒˆæˆåŠŸç‡100%ã€ã‚«ãƒãƒ¬ãƒƒã‚¸83%ä»¥ä¸Šã€å®Ÿè¡Œæ™‚é–“10åˆ†ä»¥å†…ï¼ˆGPUãƒ†ã‚¹ãƒˆé™¤ãï¼‰

### 6.1 ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™

| ã‚«ãƒ†ã‚´ãƒª | ãƒ•ã‚¡ã‚¤ãƒ« | ãƒ†ã‚¹ãƒˆæ•° | ã‚«ãƒãƒ¬ãƒƒã‚¸ç›®æ¨™ | å„ªå…ˆåº¦ |
|---------|---------|---------|--------------|--------|
| **LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** |
| CharacterFineTuning | `test_lora_tuning.py` | 45ä»¶ | 85%ä»¥ä¸Š | ğŸŸ¡ Medium |
| **æœ€çµ‚çµ±åˆãƒ†ã‚¹ãƒˆ** |
| Phase 1-7çµ±åˆ | `test_integration_phase1_7.py` | 20ä»¶ | 85%ä»¥ä¸Š | ğŸ”´ High |
| Phase 4-8çµ±åˆ | `test_integration_phase4_8.py` | 15ä»¶ | 85%ä»¥ä¸Š | ğŸ”´ High |
| ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ | `test_e2e.py` | 10ä»¶ | 80%ä»¥ä¸Š | ğŸ”´ High |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ** |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ | `test_performance.py` | 5ä»¶ | 70%ä»¥ä¸Š | ğŸŸ¡ Medium |
| **åˆè¨ˆ** | **5ãƒ•ã‚¡ã‚¤ãƒ«** | **95ä»¶ä»¥ä¸Š** | **å¹³å‡83%ä»¥ä¸Š** | - |

### 6.2 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•

#### åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆGPUä¸è¦ãƒ†ã‚¹ãƒˆã®ã¿ï¼‰
pytest tests/test_lora_tuning.py tests/test_integration_phase1_7.py tests/test_integration_phase4_8.py -v

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ããƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest tests/ \
  --cov=training.lora_tuning \
  --cov-report=html \
  --cov-report=term-missing \
  --cov-fail-under=83

# GPUãƒ†ã‚¹ãƒˆã‚’é™¤å¤–
pytest tests/ -m "not gpu" -v

# GPUãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œï¼ˆGPUç’°å¢ƒãŒå¿…è¦ï¼‰
pytest tests/ -m gpu -v

# ç‰¹å®šã®ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
pytest tests/test_lora_tuning.py::test_training_data_creation -v

# ãƒãƒ¼ã‚«ãƒ¼ã§å®Ÿè¡Œ
pytest -m unit -v              # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ã¿
pytest -m integration -v      # çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿
pytest -m e2e -v               # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã®ã¿
pytest -m "not slow" -v        # é…ã„ãƒ†ã‚¹ãƒˆã‚’é™¤å¤–
```

#### TDDã‚µã‚¤ã‚¯ãƒ«ã§ã®å®Ÿè¡Œ

```bash
# 1. ãƒ†ã‚¹ãƒˆã‚’æ›¸ã„ãŸå¾Œï¼ˆREDï¼‰
pytest tests/test_lora_tuning.py::test_training_data_creation -v
# â†’ æœŸå¾…: FAILEDï¼ˆå®Ÿè£…å‰ï¼‰

# 2. æœ€å°é™ã®å®Ÿè£…å¾Œï¼ˆGREENï¼‰
pytest tests/test_lora_tuning.py::test_training_data_creation -v
# â†’ æœŸå¾…: PASSED

# 3. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œï¼ˆREFACTORï¼‰
pytest tests/test_lora_tuning.py -v
# â†’ æœŸå¾…: å…¨ãƒ†ã‚¹ãƒˆ PASSED
```

---

## Week 1-2: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° - ãƒ†ã‚¹ãƒˆä»•æ§˜ï¼ˆTDDï¼‰

### ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: `tests/test_lora_tuning.py`

**ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹**: `TestCharacterFineTuning`

**ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ45ä»¶ã€GPUä¸è¦ï¼‰**:

#### 1. åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ï¼‰

```python
def test_fine_tuning_init_default():
    """
    Given: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    When: CharacterFineTuningã‚’åˆæœŸåŒ–
    Then: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§åˆæœŸåŒ–ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    assert tuning.base_model_name == "gpt2"
    assert tuning.lora_config.r == 8
    assert tuning.lora_config.lora_alpha == 16
    assert tuning.lora_config.lora_dropout == 0.1

def test_fine_tuning_init_custom():
    """
    Given: ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    When: CharacterFineTuningã‚’åˆæœŸåŒ–
    Then: ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§åˆæœŸåŒ–ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning(
        base_model="gpt2-medium",
        lora_r=16,
        lora_alpha=32,
        lora_dropout=0.05
    )
    
    assert tuning.base_model_name == "gpt2-medium"
    assert tuning.lora_config.r == 16
    assert tuning.lora_config.lora_alpha == 32
    assert tuning.lora_config.lora_dropout == 0.05

def test_fine_tuning_init_lora_config():
    """
    Given: CharacterFineTuningã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    When: åˆæœŸåŒ–
    Then: LoRAè¨­å®šãŒæ­£ã—ãä½œæˆã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    assert tuning.lora_config.task_type == TaskType.CAUSAL_LM
    assert tuning.lora_config.bias == "none"
    assert "c_attn" in tuning.lora_config.target_modules
```

#### 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆï¼ˆ10ä»¶ï¼‰

```python
def test_create_training_data_simple():
    """
    Given: userâ†’assistantã®ãƒšã‚¢
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: Alpacaå½¢å¼ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä½œæˆã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯", "timestamp": "2025-11-20T10:00:00Z"},
        {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼", "timestamp": "2025-11-20T10:00:01Z"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    assert len(data) == 1
    assert data[0]["instruction"] == "ã“ã‚“ã«ã¡ã¯"
    assert data[0]["input"] == ""
    assert data[0]["output"] == "ã“ã‚“ã«ã¡ã¯ï¼"

def test_create_training_data_multiple_pairs():
    """
    Given: è¤‡æ•°ã®userâ†’assistantãƒšã‚¢
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: ã™ã¹ã¦ã®ãƒšã‚¢ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user", "content": "è³ªå•1", "timestamp": "2025-11-20T10:00:00Z"},
        {"role": "assistant", "content": "å›ç­”1", "timestamp": "2025-11-20T10:00:01Z"},
        {"role": "user", "content": "è³ªå•2", "timestamp": "2025-11-20T10:00:02Z"},
        {"role": "assistant", "content": "å›ç­”2", "timestamp": "2025-11-20T10:00:03Z"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    assert len(data) == 2
    assert data[0]["instruction"] == "è³ªå•1"
    assert data[1]["instruction"] == "è³ªå•2"

def test_create_training_data_no_assistant_response():
    """
    Given: userãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾å¿œã™ã‚‹assistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãªã„
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: ãã®ãƒšã‚¢ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„
    """
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user", "content": "è³ªå•", "timestamp": "2025-11-20T10:00:00Z"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    assert len(data) == 0

def test_create_training_data_empty_conversations():
    """
    Given: ç©ºã®ä¼šè©±å±¥æ­´
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: ç©ºã®ãƒªã‚¹ãƒˆãŒè¿”ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    data = tuning._create_training_data([])
    
    assert len(data) == 0

def test_create_training_data_timestamp_order():
    """
    Given: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ã§ãªã„ä¼šè©±å±¥æ­´
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ã«å‡¦ç†ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user", "content": "è³ªå•1", "timestamp": "2025-11-20T10:00:02Z"},
        {"role": "assistant", "content": "å›ç­”1", "timestamp": "2025-11-20T10:00:03Z"},
        {"role": "user", "content": "è³ªå•2", "timestamp": "2025-11-20T10:00:00Z"},
        {"role": "assistant", "content": "å›ç­”2", "timestamp": "2025-11-20T10:00:01Z"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ã«å‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    assert len(data) >= 0  # å®Ÿè£…ã«å¿œã˜ã¦æ¤œè¨¼
```

#### 3. ä¼šè©±å±¥æ­´åé›†ãƒ†ã‚¹ãƒˆï¼ˆ8ä»¶ï¼‰

```python
def test_collect_conversations():
    """
    Given: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã¨æ—¥æ•°
    When: _collect_conversations()ã‚’å‘¼ã³å‡ºã™
    Then: æŒ‡å®šæœŸé–“ã®ä¼šè©±å±¥æ­´ãŒåé›†ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch('services.chat_service.ChatService') as mock_service:
        mock_instance = Mock()
        mock_instance.get_history = Mock(return_value=[
            {"role": "user", "content": "è³ªå•", "timestamp": "2025-11-20T10:00:00Z"}
        ])
        mock_service.return_value = mock_instance
        
        conversations = tuning._collect_conversations("lumina", days=30)
        
        assert len(conversations) == 1
        mock_instance.get_history.assert_called_once()

def test_collect_conversations_empty_result():
    """
    Given: ä¼šè©±å±¥æ­´ãŒå­˜åœ¨ã—ãªã„
    When: _collect_conversations()ã‚’å‘¼ã³å‡ºã™
    Then: ç©ºã®ãƒªã‚¹ãƒˆãŒè¿”ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch('services.chat_service.ChatService') as mock_service:
        mock_instance = Mock()
        mock_instance.get_history = Mock(return_value=[])
        mock_service.return_value = mock_instance
        
        conversations = tuning._collect_conversations("lumina", days=30)
        
        assert len(conversations) == 0
```

#### 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆ8ä»¶ã€ãƒ¢ãƒƒã‚¯ä½¿ç”¨ï¼‰

```python
def test_fine_tune_character_insufficient_data():
    """
    Given: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒ100ä»¶æœªæº€
    When: fine_tune_character()ã‚’å‘¼ã³å‡ºã™
    Then: ValueErrorãŒç™ºç”Ÿã™ã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch.object(tuning, '_collect_conversations', return_value=[{}] * 50):
        with pytest.raises(ValueError, match="è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³"):
            tuning.fine_tune_character("lumina")

def test_fine_tune_character_sufficient_data():
    """
    Given: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒ100ä»¶ä»¥ä¸Š
    When: fine_tune_character()ã‚’å‘¼ã³å‡ºã™
    Then: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿè¡Œã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    # ãƒ¢ãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—
    with patch.object(tuning, '_collect_conversations', return_value=[{}] * 100):
        with patch('transformers.AutoModelForCausalLM.from_pretrained') as mock_model:
            with patch('transformers.AutoTokenizer.from_pretrained') as mock_tokenizer:
                with patch('peft.get_peft_model') as mock_peft:
                    with patch('datasets.Dataset.from_list') as mock_dataset:
                        mock_model.return_value = Mock()
                        mock_tokenizer.return_value = Mock(pad_token=None, eos_token="<eos>")
                        mock_peft.return_value = Mock()
                        mock_dataset.return_value = Mock()
                        
                        # å®Ÿè£…ã«å¿œã˜ã¦æ¤œè¨¼
                        # result = tuning.fine_tune_character("lumina")
```

#### 5. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆ4ä»¶ï¼‰

```python
def test_load_lora_model_not_found():
    """
    Given: å­˜åœ¨ã—ãªã„LoRAãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
    When: load_lora_model()ã‚’å‘¼ã³å‡ºã™
    Then: FileNotFoundErrorãŒç™ºç”Ÿã™ã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch('os.path.exists', return_value=False):
        with pytest.raises(FileNotFoundError, match="LoRAãƒ¢ãƒ‡ãƒ«ãªã—"):
            tuning.load_lora_model("nonexistent_character")

def test_load_lora_model_success():
    """
    Given: å­˜åœ¨ã™ã‚‹LoRAãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
    When: load_lora_model()ã‚’å‘¼ã³å‡ºã™
    Then: ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch('os.path.exists', return_value=True):
        with patch('transformers.AutoModelForCausalLM.from_pretrained') as mock_base:
            with patch('peft.PeftModel.from_pretrained') as mock_peft:
                mock_base.return_value = Mock()
                mock_peft.return_value = Mock()
                
                model = tuning.load_lora_model("lumina")
                
                assert model is not None
```

#### 6. ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»ç•°å¸¸ç³»ãƒ†ã‚¹ãƒˆï¼ˆè¿½åŠ : 10ä»¶ï¼‰

```python
def test_create_training_data_missing_fields():
    """
    Given: å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ¬ ã‘ã¦ã„ã‚‹ä¼šè©±
    When: _create_training_data()ã‚’å‘¼ã³å‡ºã™
    Then: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ï¼‰
    """
    tuning = CharacterFineTuning()
    
    conversations = [
        {"role": "user"},  # contentãŒæ¬ ã‘ã¦ã„ã‚‹
        {"role": "assistant", "content": "å›ç­”"}
    ]
    
    data = tuning._create_training_data(conversations)
    
    # å®Ÿè£…ã«å¿œã˜ã¦æ¤œè¨¼
    assert isinstance(data, list)

def test_collect_conversations_invalid_days():
    """
    Given: è² ã®æ—¥æ•°
    When: _collect_conversations()ã‚’å‘¼ã³å‡ºã™
    Then: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ï¼ˆã¾ãŸã¯0ä»¶ãŒè¿”ã•ã‚Œã‚‹ï¼‰
    """
    tuning = CharacterFineTuning()
    
    # å®Ÿè£…ã«å¿œã˜ã¦ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ç©ºã®ãƒªã‚¹ãƒˆ
    with pytest.raises((ValueError, TypeError)):
        tuning._collect_conversations("lumina", days=-1)
```

#### 7. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆï¼ˆè¿½åŠ : 5ä»¶ï¼‰

```python
@pytest.mark.parametrize("lora_r,lora_alpha", [
    (4, 8),
    (8, 16),
    (16, 32),
    (32, 64),
])
def test_fine_tuning_various_lora_configs(lora_r, lora_alpha):
    """
    Given: æ§˜ã€…ãªLoRAè¨­å®š
    When: CharacterFineTuningã‚’åˆæœŸåŒ–
    Then: ã™ã¹ã¦ã®è¨­å®šã§æ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã‚‹
    """
    tuning = CharacterFineTuning(lora_r=lora_r, lora_alpha=lora_alpha)
    
    assert tuning.lora_config.r == lora_r
    assert tuning.lora_config.lora_alpha == lora_alpha

@pytest.mark.parametrize("character", ["lumina", "clarisse", "nox"])
def test_collect_conversations_all_characters(character):
    """
    Given: å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼
    When: _collect_conversations()ã‚’å‘¼ã³å‡ºã™
    Then: ã™ã¹ã¦ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§æ­£ã—ãå‹•ä½œã™ã‚‹
    """
    tuning = CharacterFineTuning()
    
    with patch('services.chat_service.ChatService') as mock_service:
        mock_instance = Mock()
        mock_instance.get_history = Mock(return_value=[])
        mock_service.return_value = mock_instance
        
        conversations = tuning._collect_conversations(character, days=30)
        
        assert isinstance(conversations, list)
        mock_instance.get_history.assert_called_once()
```

---

## 7. æˆæœç‰©

### 7.1 å®Ÿè£…ã‚³ãƒ¼ãƒ‰

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**:
- `training/lora_tuning.py` (400è¡Œ)
- **åˆè¨ˆ**: 400è¡Œ

### 7.2 ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰

**æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«**:
- `tests/test_lora_tuning.py` (45ä»¶)
  - åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ: 5ä»¶
  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆ: 10ä»¶
  - ä¼šè©±å±¥æ­´åé›†ãƒ†ã‚¹ãƒˆ: 8ä»¶
  - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œãƒ†ã‚¹ãƒˆ: 8ä»¶
  - ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ: 4ä»¶
  - ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»ç•°å¸¸ç³»ãƒ†ã‚¹ãƒˆ: 10ä»¶
  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆ: 5ä»¶
- `tests/test_integration_phase1_7.py` (20ä»¶)
  - LangGraphã‚³ã‚¢çµ±åˆ: 5ä»¶
  - é€£æƒ³è¨˜æ†¶ãƒ»æ„Ÿæƒ…çµ±åˆ: 5ä»¶
  - å¯¾è©±ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»è‡ªå·±çœå¯Ÿçµ±åˆ: 5ä»¶
  - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·ãƒ»MCPçµ±åˆ: 5ä»¶
- `tests/test_integration_phase4_8.py` (15ä»¶)
  - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ä¼šè©±: 3ä»¶
  - é€£æƒ³è¨˜æ†¶çµ±åˆ: 2ä»¶
  - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æˆé•·çµ±åˆ: 2ä»¶
  - MCP Serverçµ±åˆ: 2ä»¶
  - 3Då¯è¦–åŒ–çµ±åˆ: 2ä»¶
  - è‡ªå¾‹ã‚µãƒ¼ãƒçµ±åˆ: 2ä»¶
  - å…¨ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ: 2ä»¶
- `tests/test_e2e.py` (10ä»¶)
  - å®Œå…¨ãªä¼šè©±ãƒ•ãƒ­ãƒ¼: 3ä»¶
  - è¤‡æ•°ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼: 2ä»¶
  - é•·æ™‚é–“ä¼šè©±: 2ä»¶
  - ã‚¨ãƒ©ãƒ¼å›å¾©: 2ä»¶
  - å¤§é‡ãƒ‡ãƒ¼ã‚¿: 1ä»¶
- `tests/test_performance.py` (5ä»¶)
  - å¿œç­”æ™‚é–“: 2ä»¶
  - æ¤œç´¢æ™‚é–“: 1ä»¶
  - ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡: 2ä»¶
- `tests/fixtures/lora_tuning_fixtures.py`: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å®šç¾©
- **åˆè¨ˆ**: 95ä»¶ä»¥ä¸Šï¼ˆã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒ†ã‚¹ãƒˆå«ã‚€ï¼‰

### 7.3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- `docks/å®Œäº†å ±å‘Š/Phase8_å®Œäº†ã‚µãƒãƒªãƒ¼.md`
- `docks/å®Œäº†å ±å‘Š/Phase4-8_æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ.md`
- ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ v4.0.0

### 7.4 ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³

- [ ] LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ä½œç¢ºèª
- [ ] å…¨Phaseçµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ
- [ ] ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†
- [ ] å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼ˆ95ä»¶ä»¥ä¸Šï¼‰
- [ ] ã‚«ãƒãƒ¬ãƒƒã‚¸ > 83%
- [ ] v4.0.0ãƒªãƒªãƒ¼ã‚¹

---

## 8. Phase 8æˆåŠŸåŸºæº–

### TDDå®Ÿè£…ã®æˆåŠŸåŸºæº–

**å¿…é ˆè¦ä»¶**:
- âœ… **ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆ**: å…¨æ©Ÿèƒ½ãŒãƒ†ã‚¹ãƒˆé§†å‹•ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹
- âœ… **ãƒ†ã‚¹ãƒˆæˆåŠŸç‡**: 100%ï¼ˆå…¨95ä»¶ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸï¼‰
- âœ… **ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸**: 83%ä»¥ä¸Šï¼ˆå¹³å‡ï¼‰
- âœ… **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“**: å…¨ãƒ†ã‚¹ãƒˆ10åˆ†ä»¥å†…ï¼ˆGPUãƒ†ã‚¹ãƒˆé™¤ãï¼‰
- âœ… **ãƒ†ã‚¹ãƒˆç‹¬ç«‹æ€§**: å„ãƒ†ã‚¹ãƒˆã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½
- âœ… **ãƒ¢ãƒƒã‚¯ä½¿ç”¨**: å¤–éƒ¨ä¾å­˜ï¼ˆGPUã€ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ç­‰ï¼‰ã¯ãƒ¢ãƒƒã‚¯ã§åˆ†é›¢

**TDDã‚µã‚¤ã‚¯ãƒ«éµå®ˆ**:
- âœ… RED: å®Ÿè£…å‰ã«ãƒ†ã‚¹ãƒˆã‚’æ›¸ã„ã¦ã„ã‚‹
- âœ… GREEN: æœ€å°é™ã®å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã‚’é€šã—ã¦ã„ã‚‹
- âœ… REFACTOR: ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã‚‚ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¦ã„ã‚‹

### å®šé‡ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|------|--------|----------|
| **ãƒ†ã‚¹ãƒˆæˆåŠŸç‡** | **100%** | pytestï¼ˆå…¨95ä»¶ä»¥ä¸Šï¼‰ |
| **ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸** | **83%ä»¥ä¸Š** | pytest-cov |
| **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“** | **< 10åˆ†** | pytest --durationsï¼ˆGPUãƒ†ã‚¹ãƒˆé™¤ãï¼‰ |
| LoRAè¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆæ™‚é–“ | < 1åˆ† | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ |
| çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ | < 5åˆ† | çµ±åˆãƒ†ã‚¹ãƒˆ |
| ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ | < 3åˆ† | E2Eãƒ†ã‚¹ãƒˆ |

### å®šæ€§ç›®æ¨™

âœ… **TDDå®Ÿè£…å®Œäº†**: å…¨æ©Ÿèƒ½ãŒãƒ†ã‚¹ãƒˆé§†å‹•ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹
âœ… **ãƒ†ã‚¹ãƒˆä»•æ§˜å®Œå‚™**: å…¨95ä»¶ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹
âœ… **LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‹•ä½œ**: æœˆæ¬¡ãƒãƒƒãƒå‡¦ç†
âœ… **å…¨Phaseçµ±åˆå®Œäº†**: Phase 1-7ã®å…¨æ©Ÿèƒ½ãŒçµ±åˆã•ã‚Œã¦ã„ã‚‹
âœ… **ãƒªãƒªãƒ¼ã‚¹æº–å‚™å®Œäº†**: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†å®Œäº†

---

**Phase 8 å®Ÿè£…å®Œäº†**: LlmMultiChat3 v4.0.0å®Œæˆï¼